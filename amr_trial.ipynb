{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNNNLdOrVdYofU49IMrXQ13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bb6a55b33cc49c4806f7a118f38b121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eff40e48f87481eacf3bdadc88dec32",
              "IPY_MODEL_65f1af273ff84b3f93bfba5847bf96e1",
              "IPY_MODEL_5a3cd10ca2ee46809d45e52dc5233e65"
            ],
            "layout": "IPY_MODEL_d665e75b7ab844c181e9f2a3adbd4f15"
          }
        },
        "5eff40e48f87481eacf3bdadc88dec32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7687303efe4243feb53c394b21410ec1",
            "placeholder": "​",
            "style": "IPY_MODEL_07c791e24bab4c30b9c54c6c3f4b429a",
            "value": "spiece.model: 100%"
          }
        },
        "65f1af273ff84b3f93bfba5847bf96e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae5b4c5995c4c1782f5da13339f097d",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df489a1bf9354c2484ef05281bebbff1",
            "value": 791656
          }
        },
        "5a3cd10ca2ee46809d45e52dc5233e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6667cdb929444a828271e559b26d363d",
            "placeholder": "​",
            "style": "IPY_MODEL_8c3fc3f9510f4b79aa18ddd4f9febfb6",
            "value": " 792k/792k [00:00&lt;00:00, 12.0MB/s]"
          }
        },
        "d665e75b7ab844c181e9f2a3adbd4f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7687303efe4243feb53c394b21410ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c791e24bab4c30b9c54c6c3f4b429a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ae5b4c5995c4c1782f5da13339f097d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df489a1bf9354c2484ef05281bebbff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6667cdb929444a828271e559b26d363d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3fc3f9510f4b79aa18ddd4f9febfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afad1e39b6c74be699f337a774a483cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_012e490861f349a29af548c8f92fa143",
              "IPY_MODEL_4e719b0f04334d769f4b83ed8324491e",
              "IPY_MODEL_baebc33e6a3843e78613de4220bafaea"
            ],
            "layout": "IPY_MODEL_7ac7140da71f443a9ee1bd3ef07f80f0"
          }
        },
        "012e490861f349a29af548c8f92fa143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affed3b859364e03baf55040f7ad7df8",
            "placeholder": "​",
            "style": "IPY_MODEL_d8737b02971c4ef09c85092c5a257e2e",
            "value": "tokenizer.json: 100%"
          }
        },
        "4e719b0f04334d769f4b83ed8324491e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0442b152e3bd4c1d9352984846828ba4",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6b7c7a94cb6422184eaf7093f52dcb4",
            "value": 1389353
          }
        },
        "baebc33e6a3843e78613de4220bafaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b1bd1528596433392c866362ad9fb19",
            "placeholder": "​",
            "style": "IPY_MODEL_eda26f3fc3df4dc89a6884fd70f80c42",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 35.8MB/s]"
          }
        },
        "7ac7140da71f443a9ee1bd3ef07f80f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "affed3b859364e03baf55040f7ad7df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8737b02971c4ef09c85092c5a257e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0442b152e3bd4c1d9352984846828ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b7c7a94cb6422184eaf7093f52dcb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b1bd1528596433392c866362ad9fb19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eda26f3fc3df4dc89a6884fd70f80c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9ebc67d1f7241fbbc9a3b0487bf3e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe78da53d7094cfa81ddaa6e606291bf",
              "IPY_MODEL_bc9a8fdf8484423ca6a4bcbe4a6b95ba",
              "IPY_MODEL_da33ca40ff7144e8a3c688cb0e469e6d"
            ],
            "layout": "IPY_MODEL_bc5aea863d33461aac97b2caa10cdf81"
          }
        },
        "fe78da53d7094cfa81ddaa6e606291bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5879cc67738418c8ccec038bd5dae59",
            "placeholder": "​",
            "style": "IPY_MODEL_7310384979b94d2cb004857796cfd78e",
            "value": "config.json: 100%"
          }
        },
        "bc9a8fdf8484423ca6a4bcbe4a6b95ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5955ae0415b54fa9bbbfa5e8038a7edc",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_046ff847be5b4ea981c9efcc9c1ef12e",
            "value": 1208
          }
        },
        "da33ca40ff7144e8a3c688cb0e469e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831e20812f0e41ddad76f0a740a97149",
            "placeholder": "​",
            "style": "IPY_MODEL_99ed042ffefe45dbb65354e566db0914",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 92.3kB/s]"
          }
        },
        "bc5aea863d33461aac97b2caa10cdf81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5879cc67738418c8ccec038bd5dae59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7310384979b94d2cb004857796cfd78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5955ae0415b54fa9bbbfa5e8038a7edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "046ff847be5b4ea981c9efcc9c1ef12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "831e20812f0e41ddad76f0a740a97149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ed042ffefe45dbb65354e566db0914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5384daecea604e9198a5efa48da27410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdfd8553e2ca42e59e3826fa658dafc6",
              "IPY_MODEL_4aad8c87ae564553b59407802b126617",
              "IPY_MODEL_d66b5f962ba7467fa1d2accbdee4fc0b"
            ],
            "layout": "IPY_MODEL_2796228ebb084c1ab0684e29541eb3b7"
          }
        },
        "fdfd8553e2ca42e59e3826fa658dafc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09237c780a6b4fda8b7bdbed2a765f01",
            "placeholder": "​",
            "style": "IPY_MODEL_106375c8055643f7937050e43ad5f292",
            "value": "model.safetensors: 100%"
          }
        },
        "4aad8c87ae564553b59407802b126617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4845c19cbc04ca9971881bbbc68ca74",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_077574d6df19478ba04d7dac95aa9371",
            "value": 891646390
          }
        },
        "d66b5f962ba7467fa1d2accbdee4fc0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9266600848234606ac48c1003a533927",
            "placeholder": "​",
            "style": "IPY_MODEL_92f732ee7c904cebb09c43e1365c05ad",
            "value": " 892M/892M [00:09&lt;00:00, 93.6MB/s]"
          }
        },
        "2796228ebb084c1ab0684e29541eb3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09237c780a6b4fda8b7bdbed2a765f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106375c8055643f7937050e43ad5f292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4845c19cbc04ca9971881bbbc68ca74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "077574d6df19478ba04d7dac95aa9371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9266600848234606ac48c1003a533927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f732ee7c904cebb09c43e1365c05ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3d84570f5c04a70866227374392e773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7787b7396d14692a45c2d14cc5341ce",
              "IPY_MODEL_cf779795be584713a4ed8075b5b249a0",
              "IPY_MODEL_ad3df42861ff471da58589f794751025"
            ],
            "layout": "IPY_MODEL_a159a72bb32e41c2a790f873412825f7"
          }
        },
        "d7787b7396d14692a45c2d14cc5341ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b13be7532d64223a4f4e9bbdfbcd877",
            "placeholder": "​",
            "style": "IPY_MODEL_8be9cca26236493eb50d1bbe4b6674ec",
            "value": "generation_config.json: 100%"
          }
        },
        "cf779795be584713a4ed8075b5b249a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da36e38de4254c8fad157859675996c1",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1191fb069199482892c2cbfcd8c49839",
            "value": 147
          }
        },
        "ad3df42861ff471da58589f794751025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5fcb5d4e5249168f8edfd3660ace34",
            "placeholder": "​",
            "style": "IPY_MODEL_a5f831e8b3d84406ac4742f646df8696",
            "value": " 147/147 [00:00&lt;00:00, 13.5kB/s]"
          }
        },
        "a159a72bb32e41c2a790f873412825f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b13be7532d64223a4f4e9bbdfbcd877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8be9cca26236493eb50d1bbe4b6674ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da36e38de4254c8fad157859675996c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1191fb069199482892c2cbfcd8c49839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d5fcb5d4e5249168f8edfd3660ace34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f831e8b3d84406ac4742f646df8696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxrlton/AMR/blob/main/amr_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import penman\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Set"
      ],
      "metadata": {
        "id": "oBIAlFa87igJ"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AMRPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.ne_type_mapping = {\n",
        "            'person': 'person',\n",
        "            'team': 'organization',\n",
        "            'country': 'location',\n",
        "            'city': 'location',\n",
        "            'state': 'location',\n",
        "            'organization': 'organization',\n",
        "            'company': 'organization',\n",
        "            'government-organization': 'organization',\n",
        "            'group': 'organization'\n",
        "        }\n",
        "        self.anon_counter = defaultdict(int)\n",
        "\n",
        "    def preprocess_amr(self, amr_str: str) -> Tuple[str, Dict]:\n",
        "        try:\n",
        "            # Initialize entity map\n",
        "            entity_map = {}\n",
        "\n",
        "            # Handle multi-line AMR string\n",
        "            amr_str = amr_str.replace('\\n', ' ')\n",
        "\n",
        "            # Process named entities\n",
        "            for ne_type in self.ne_type_mapping:\n",
        "                # Updated pattern to match the exact structure\n",
        "                pattern = fr'\\(\\s*(?:z\\d+\\s*/\\s*)?{ne_type}[^)]*:name\\s*\\([^)]*:op1\\s*\"([^\"]+)\"'\n",
        "                matches = list(re.finditer(pattern, amr_str))\n",
        "\n",
        "                for match in matches:\n",
        "                    entity_name = match.group(1)\n",
        "                    coarse_type = self.ne_type_mapping[ne_type]\n",
        "                    anon_id = f\"{coarse_type}_{self.anon_counter[coarse_type]}\"\n",
        "                    self.anon_counter[coarse_type] += 1\n",
        "\n",
        "                    # Store the mapping\n",
        "                    entity_map[anon_id] = entity_name\n",
        "\n",
        "                    # Create replacement pattern\n",
        "                    name_section = f':name\\\\s*\\\\([^)]*:op1\\\\s*\"{entity_name}\"[^)]*\\\\)'\n",
        "                    replacement = f' {anon_id}'\n",
        "\n",
        "                    # Replace the name section with the anonymous ID\n",
        "                    amr_str = re.sub(name_section, replacement, amr_str)\n",
        "\n",
        "            # Process dates\n",
        "            amr_str = re.sub(r':year\\s+(\\d{4})', r':year year_\\1', amr_str)\n",
        "            amr_str = re.sub(r':month\\s+(\\d{1,2})', r':month month_\\1', amr_str)\n",
        "            amr_str = re.sub(r':day\\s+(\\d{1,2})', r':day day_\\1', amr_str)\n",
        "\n",
        "            # Clean up variables\n",
        "            amr_str = re.sub(r'\\(z\\d+\\s*/', '(', amr_str)\n",
        "            amr_str = re.sub(r'\\sz\\d+\\s', ' ', amr_str)\n",
        "            amr_str = re.sub(r'\\s*\\/\\s*', ' ', amr_str)\n",
        "\n",
        "            return amr_str, entity_map\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocess_amr: {e}\")\n",
        "            return amr_str, {}\n",
        "\n",
        "# # Test with specific examples\n",
        "# test_cases = [\n",
        "#     \"\"\"(z0 / and\n",
        "#     :op1 (z1 / beat-03\n",
        "#              :ARG0 (z2 / team\n",
        "#                        :name (z3 / name\n",
        "#                                  :op1 \"Carlton\"))\n",
        "#              :ARG1 (z4 / team\n",
        "#                        :name (z5 / name\n",
        "#                                  :op1 \"Melbourne\"))\n",
        "#              :time (z6 / date-entity\n",
        "#                        :year 2016)))\"\"\",\n",
        "\n",
        "#     \"\"\"(z0 / urge-01\n",
        "#     :ARG0 (z1 / person\n",
        "#               :name (z2 / name\n",
        "#                         :op1 \"Gillum\")\n",
        "#               :medium (z3 / television))\n",
        "#     :ARG1 (z4 / person\n",
        "#               :ARG0-of (z5 / reside-01)))\"\"\"\n",
        "# ]\n",
        "\n",
        "# preprocessor = AMRPreprocessor()\n",
        "\n",
        "# print(\"Testing examples:\")\n",
        "# for i, test_amr in enumerate(test_cases):\n",
        "#     print(f\"\\nTest case {i + 1}:\")\n",
        "#     print(\"Original:\", test_amr)\n",
        "#     processed_amr, entity_map = preprocessor.preprocess_amr(test_amr)\n",
        "#     print(\"\\nProcessed:\", processed_amr)\n",
        "#     print(\"Entity Map:\", entity_map)\n",
        "#     print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "n7lvpG2kRqaH"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Tverous/anli_amr_new2\", split=\"train\")"
      ],
      "metadata": {
        "id": "7me2y8xOJ20p"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset loaded. Size: {len(dataset)}\")"
      ],
      "metadata": {
        "id": "LWr2Jj9zKN6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd3461d-344c-4acc-ba7e-5c2d4627b3a8"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Size: 100459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = []"
      ],
      "metadata": {
        "id": "bIiIO9XJNZ41"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, example in enumerate(dataset):\n",
        "    try:\n",
        "        entry = {\n",
        "            'premise': example.get('premise', ''),\n",
        "            'hypothesis': example.get('hypothesis', ''),\n",
        "            'label': example.get('label', -1),\n",
        "            'amr': example.get('amr_penman', '')\n",
        "        }\n",
        "        processed_data.append(entry)\n",
        "\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"Processed {idx} examples...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example {idx}: {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "rZB25EjD56HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d0494d-34ea-410c-d37b-e189a5ac688b"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 examples...\n",
            "Processed 1000 examples...\n",
            "Processed 2000 examples...\n",
            "Processed 3000 examples...\n",
            "Processed 4000 examples...\n",
            "Processed 5000 examples...\n",
            "Processed 6000 examples...\n",
            "Processed 7000 examples...\n",
            "Processed 8000 examples...\n",
            "Processed 9000 examples...\n",
            "Processed 10000 examples...\n",
            "Processed 11000 examples...\n",
            "Processed 12000 examples...\n",
            "Processed 13000 examples...\n",
            "Processed 14000 examples...\n",
            "Processed 15000 examples...\n",
            "Processed 16000 examples...\n",
            "Processed 17000 examples...\n",
            "Processed 18000 examples...\n",
            "Processed 19000 examples...\n",
            "Processed 20000 examples...\n",
            "Processed 21000 examples...\n",
            "Processed 22000 examples...\n",
            "Processed 23000 examples...\n",
            "Processed 24000 examples...\n",
            "Processed 25000 examples...\n",
            "Processed 26000 examples...\n",
            "Processed 27000 examples...\n",
            "Processed 28000 examples...\n",
            "Processed 29000 examples...\n",
            "Processed 30000 examples...\n",
            "Processed 31000 examples...\n",
            "Processed 32000 examples...\n",
            "Processed 33000 examples...\n",
            "Processed 34000 examples...\n",
            "Processed 35000 examples...\n",
            "Processed 36000 examples...\n",
            "Processed 37000 examples...\n",
            "Processed 38000 examples...\n",
            "Processed 39000 examples...\n",
            "Processed 40000 examples...\n",
            "Processed 41000 examples...\n",
            "Processed 42000 examples...\n",
            "Processed 43000 examples...\n",
            "Processed 44000 examples...\n",
            "Processed 45000 examples...\n",
            "Processed 46000 examples...\n",
            "Processed 47000 examples...\n",
            "Processed 48000 examples...\n",
            "Processed 49000 examples...\n",
            "Processed 50000 examples...\n",
            "Processed 51000 examples...\n",
            "Processed 52000 examples...\n",
            "Processed 53000 examples...\n",
            "Processed 54000 examples...\n",
            "Processed 55000 examples...\n",
            "Processed 56000 examples...\n",
            "Processed 57000 examples...\n",
            "Processed 58000 examples...\n",
            "Processed 59000 examples...\n",
            "Processed 60000 examples...\n",
            "Processed 61000 examples...\n",
            "Processed 62000 examples...\n",
            "Processed 63000 examples...\n",
            "Processed 64000 examples...\n",
            "Processed 65000 examples...\n",
            "Processed 66000 examples...\n",
            "Processed 67000 examples...\n",
            "Processed 68000 examples...\n",
            "Processed 69000 examples...\n",
            "Processed 70000 examples...\n",
            "Processed 71000 examples...\n",
            "Processed 72000 examples...\n",
            "Processed 73000 examples...\n",
            "Processed 74000 examples...\n",
            "Processed 75000 examples...\n",
            "Processed 76000 examples...\n",
            "Processed 77000 examples...\n",
            "Processed 78000 examples...\n",
            "Processed 79000 examples...\n",
            "Processed 80000 examples...\n",
            "Processed 81000 examples...\n",
            "Processed 82000 examples...\n",
            "Processed 83000 examples...\n",
            "Processed 84000 examples...\n",
            "Processed 85000 examples...\n",
            "Processed 86000 examples...\n",
            "Processed 87000 examples...\n",
            "Processed 88000 examples...\n",
            "Processed 89000 examples...\n",
            "Processed 90000 examples...\n",
            "Processed 91000 examples...\n",
            "Processed 92000 examples...\n",
            "Processed 93000 examples...\n",
            "Processed 94000 examples...\n",
            "Processed 95000 examples...\n",
            "Processed 96000 examples...\n",
            "Processed 97000 examples...\n",
            "Processed 98000 examples...\n",
            "Processed 99000 examples...\n",
            "Processed 100000 examples...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(processed_data)"
      ],
      "metadata": {
        "id": "1N3HCrlZ7pcg"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "waOG4tnQ9QUj",
        "outputId": "a8964a2f-1665-42db-bb6c-c0c6ab8e2e30"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  premise  \\\n",
              "0       TOKYO, Dec 18 (Reuters) - Japan’s Shionogi & C...   \n",
              "1       Tallahassee Mayor and Democratic gubernatorial...   \n",
              "2       MELBOURNE will look to avoid stumbling against...   \n",
              "3       by Ted Raymond, Newstalk 580 CFRA A stretch of...   \n",
              "4       Drivers are reporting heavy traffic on the nor...   \n",
              "...                                                   ...   \n",
              "100454  A U.S. soldier accused of participating in the...   \n",
              "100455  Figures released today show that the number of...   \n",
              "100456  For Bechtolsheim, who designed the prototype f...   \n",
              "100457  Figures released today show that the number of...   \n",
              "100458  Tenzin Gyatso, the 14th Dalai Lama, has vowed ...   \n",
              "\n",
              "                                               hypothesis  label  \\\n",
              "0               The article was written on December 18th.      0   \n",
              "1       Gillum was on TV urging residents to stay out ...      0   \n",
              "2       Carlton beat Melbourne in 2016 and will attemp...      0   \n",
              "3       The road was closed for more than two hours af...      0   \n",
              "4                              Its advisible to slow down      0   \n",
              "...                                                   ...    ...   \n",
              "100454  The Soldier committed these crimes in the unit...      1   \n",
              "100455  there is a continuous decline in the number of...      1   \n",
              "100456        Bechtolsheim is experienced with computers.      0   \n",
              "100457  figures shows New Zealanders are the lowest or...      1   \n",
              "100458                Some tibetans welcome Chinese rule.      1   \n",
              "\n",
              "                                                      amr  \n",
              "0       (z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...  \n",
              "1       (z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...  \n",
              "2       (z0 / and\\n    :op1 (z1 / beat-03\\n           ...  \n",
              "3       (z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...  \n",
              "4         (z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))  \n",
              "...                                                   ...  \n",
              "100454  (z0 / commit-02\\n    :ARG0 (z1 / person\\n     ...  \n",
              "100455  (z0 / decline-01\\n    :ARG1 (z1 / number\\n    ...  \n",
              "100456  (z0 / experience-01\\n    :ARG0 (z1 / person\\n ...  \n",
              "100457  (z0 / show-01\\n    :ARG0 (z1 / figure)\\n    :A...  \n",
              "100458  (z0 / welcome-01\\n    :ARG0 (z1 / person\\n    ...  \n",
              "\n",
              "[100459 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c58901c4-de61-40b7-8d02-7cfe96b965a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "      <th>amr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TOKYO, Dec 18 (Reuters) - Japan’s Shionogi &amp; C...</td>\n",
              "      <td>The article was written on December 18th.</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tallahassee Mayor and Democratic gubernatorial...</td>\n",
              "      <td>Gillum was on TV urging residents to stay out ...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MELBOURNE will look to avoid stumbling against...</td>\n",
              "      <td>Carlton beat Melbourne in 2016 and will attemp...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / and\\n    :op1 (z1 / beat-03\\n           ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>by Ted Raymond, Newstalk 580 CFRA A stretch of...</td>\n",
              "      <td>The road was closed for more than two hours af...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Drivers are reporting heavy traffic on the nor...</td>\n",
              "      <td>Its advisible to slow down</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100454</th>\n",
              "      <td>A U.S. soldier accused of participating in the...</td>\n",
              "      <td>The Soldier committed these crimes in the unit...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / commit-02\\n    :ARG0 (z1 / person\\n     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100455</th>\n",
              "      <td>Figures released today show that the number of...</td>\n",
              "      <td>there is a continuous decline in the number of...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / decline-01\\n    :ARG1 (z1 / number\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100456</th>\n",
              "      <td>For Bechtolsheim, who designed the prototype f...</td>\n",
              "      <td>Bechtolsheim is experienced with computers.</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / experience-01\\n    :ARG0 (z1 / person\\n ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100457</th>\n",
              "      <td>Figures released today show that the number of...</td>\n",
              "      <td>figures shows New Zealanders are the lowest or...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / show-01\\n    :ARG0 (z1 / figure)\\n    :A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100458</th>\n",
              "      <td>Tenzin Gyatso, the 14th Dalai Lama, has vowed ...</td>\n",
              "      <td>Some tibetans welcome Chinese rule.</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / welcome-01\\n    :ARG0 (z1 / person\\n    ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100459 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c58901c4-de61-40b7-8d02-7cfe96b965a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c58901c4-de61-40b7-8d02-7cfe96b965a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c58901c4-de61-40b7-8d02-7cfe96b965a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f16fc115-8490-45d0-a5d2-094cc88f92e7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f16fc115-8490-45d0-a5d2-094cc88f92e7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f16fc115-8490-45d0-a5d2-094cc88f92e7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f338ed0d-846d-4641-9ba2-1a0e0cf52629\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f338ed0d-846d-4641-9ba2-1a0e0cf52629 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = AMRPreprocessor()"
      ],
      "metadata": {
        "id": "m9aJlmnQFRnp"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nProcessing AMRs...\")\n",
        "processed_amrs = []\n",
        "entity_mappings = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        processed_amr, entity_map = preprocessor.preprocess_amr(row['amr'])\n",
        "        processed_amrs.append(processed_amr)\n",
        "        entity_mappings.append(entity_map)\n",
        "\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"Preprocessed {idx} AMRs...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        processed_amrs.append(row['amr'])\n",
        "        entity_mappings.append({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0xwbYxkwo_l",
        "outputId": "8ac67780-7099-4330-855b-3274fcf3a956"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing AMRs...\n",
            "Preprocessed 0 AMRs...\n",
            "Preprocessed 1000 AMRs...\n",
            "Preprocessed 2000 AMRs...\n",
            "Preprocessed 3000 AMRs...\n",
            "Preprocessed 4000 AMRs...\n",
            "Preprocessed 5000 AMRs...\n",
            "Preprocessed 6000 AMRs...\n",
            "Preprocessed 7000 AMRs...\n",
            "Preprocessed 8000 AMRs...\n",
            "Preprocessed 9000 AMRs...\n",
            "Preprocessed 10000 AMRs...\n",
            "Preprocessed 11000 AMRs...\n",
            "Preprocessed 12000 AMRs...\n",
            "Preprocessed 13000 AMRs...\n",
            "Preprocessed 14000 AMRs...\n",
            "Preprocessed 15000 AMRs...\n",
            "Preprocessed 16000 AMRs...\n",
            "Preprocessed 17000 AMRs...\n",
            "Preprocessed 18000 AMRs...\n",
            "Preprocessed 19000 AMRs...\n",
            "Preprocessed 20000 AMRs...\n",
            "Preprocessed 21000 AMRs...\n",
            "Preprocessed 22000 AMRs...\n",
            "Preprocessed 23000 AMRs...\n",
            "Preprocessed 24000 AMRs...\n",
            "Preprocessed 25000 AMRs...\n",
            "Preprocessed 26000 AMRs...\n",
            "Preprocessed 27000 AMRs...\n",
            "Preprocessed 28000 AMRs...\n",
            "Preprocessed 29000 AMRs...\n",
            "Preprocessed 30000 AMRs...\n",
            "Preprocessed 31000 AMRs...\n",
            "Preprocessed 32000 AMRs...\n",
            "Preprocessed 33000 AMRs...\n",
            "Preprocessed 34000 AMRs...\n",
            "Preprocessed 35000 AMRs...\n",
            "Preprocessed 36000 AMRs...\n",
            "Preprocessed 37000 AMRs...\n",
            "Preprocessed 38000 AMRs...\n",
            "Preprocessed 39000 AMRs...\n",
            "Preprocessed 40000 AMRs...\n",
            "Preprocessed 41000 AMRs...\n",
            "Preprocessed 42000 AMRs...\n",
            "Preprocessed 43000 AMRs...\n",
            "Preprocessed 44000 AMRs...\n",
            "Preprocessed 45000 AMRs...\n",
            "Preprocessed 46000 AMRs...\n",
            "Preprocessed 47000 AMRs...\n",
            "Preprocessed 48000 AMRs...\n",
            "Preprocessed 49000 AMRs...\n",
            "Preprocessed 50000 AMRs...\n",
            "Preprocessed 51000 AMRs...\n",
            "Preprocessed 52000 AMRs...\n",
            "Preprocessed 53000 AMRs...\n",
            "Preprocessed 54000 AMRs...\n",
            "Preprocessed 55000 AMRs...\n",
            "Preprocessed 56000 AMRs...\n",
            "Preprocessed 57000 AMRs...\n",
            "Preprocessed 58000 AMRs...\n",
            "Preprocessed 59000 AMRs...\n",
            "Preprocessed 60000 AMRs...\n",
            "Preprocessed 61000 AMRs...\n",
            "Preprocessed 62000 AMRs...\n",
            "Preprocessed 63000 AMRs...\n",
            "Preprocessed 64000 AMRs...\n",
            "Preprocessed 65000 AMRs...\n",
            "Preprocessed 66000 AMRs...\n",
            "Preprocessed 67000 AMRs...\n",
            "Preprocessed 68000 AMRs...\n",
            "Preprocessed 69000 AMRs...\n",
            "Preprocessed 70000 AMRs...\n",
            "Preprocessed 71000 AMRs...\n",
            "Preprocessed 72000 AMRs...\n",
            "Preprocessed 73000 AMRs...\n",
            "Preprocessed 74000 AMRs...\n",
            "Preprocessed 75000 AMRs...\n",
            "Preprocessed 76000 AMRs...\n",
            "Preprocessed 77000 AMRs...\n",
            "Preprocessed 78000 AMRs...\n",
            "Preprocessed 79000 AMRs...\n",
            "Preprocessed 80000 AMRs...\n",
            "Preprocessed 81000 AMRs...\n",
            "Preprocessed 82000 AMRs...\n",
            "Preprocessed 83000 AMRs...\n",
            "Preprocessed 84000 AMRs...\n",
            "Preprocessed 85000 AMRs...\n",
            "Preprocessed 86000 AMRs...\n",
            "Preprocessed 87000 AMRs...\n",
            "Preprocessed 88000 AMRs...\n",
            "Preprocessed 89000 AMRs...\n",
            "Preprocessed 90000 AMRs...\n",
            "Preprocessed 91000 AMRs...\n",
            "Preprocessed 92000 AMRs...\n",
            "Preprocessed 93000 AMRs...\n",
            "Preprocessed 94000 AMRs...\n",
            "Preprocessed 95000 AMRs...\n",
            "Preprocessed 96000 AMRs...\n",
            "Preprocessed 97000 AMRs...\n",
            "Preprocessed 98000 AMRs...\n",
            "Preprocessed 99000 AMRs...\n",
            "Preprocessed 100000 AMRs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to DataFrame\n",
        "df['processed_amr'] = processed_amrs\n",
        "df['entity_mappings'] = entity_mappings"
      ],
      "metadata": {
        "id": "V2Q5lgbXQdNz"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "MptBh7yLTstO",
        "outputId": "bbd7efe7-1768-4b86-9e52-3cfa16455164"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  premise  \\\n",
              "0       TOKYO, Dec 18 (Reuters) - Japan’s Shionogi & C...   \n",
              "1       Tallahassee Mayor and Democratic gubernatorial...   \n",
              "2       MELBOURNE will look to avoid stumbling against...   \n",
              "3       by Ted Raymond, Newstalk 580 CFRA A stretch of...   \n",
              "4       Drivers are reporting heavy traffic on the nor...   \n",
              "...                                                   ...   \n",
              "100454  A U.S. soldier accused of participating in the...   \n",
              "100455  Figures released today show that the number of...   \n",
              "100456  For Bechtolsheim, who designed the prototype f...   \n",
              "100457  Figures released today show that the number of...   \n",
              "100458  Tenzin Gyatso, the 14th Dalai Lama, has vowed ...   \n",
              "\n",
              "                                               hypothesis  label  \\\n",
              "0               The article was written on December 18th.      0   \n",
              "1       Gillum was on TV urging residents to stay out ...      0   \n",
              "2       Carlton beat Melbourne in 2016 and will attemp...      0   \n",
              "3       The road was closed for more than two hours af...      0   \n",
              "4                              Its advisible to slow down      0   \n",
              "...                                                   ...    ...   \n",
              "100454  The Soldier committed these crimes in the unit...      1   \n",
              "100455  there is a continuous decline in the number of...      1   \n",
              "100456        Bechtolsheim is experienced with computers.      0   \n",
              "100457  figures shows New Zealanders are the lowest or...      1   \n",
              "100458                Some tibetans welcome Chinese rule.      1   \n",
              "\n",
              "                                                      amr  \\\n",
              "0       (z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...   \n",
              "1       (z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...   \n",
              "2       (z0 / and\\n    :op1 (z1 / beat-03\\n           ...   \n",
              "3       (z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...   \n",
              "4         (z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))   \n",
              "...                                                   ...   \n",
              "100454  (z0 / commit-02\\n    :ARG0 (z1 / person\\n     ...   \n",
              "100455  (z0 / decline-01\\n    :ARG1 (z1 / number\\n    ...   \n",
              "100456  (z0 / experience-01\\n    :ARG0 (z1 / person\\n ...   \n",
              "100457  (z0 / show-01\\n    :ARG0 (z1 / figure)\\n    :A...   \n",
              "100458  (z0 / welcome-01\\n    :ARG0 (z1 / person\\n    ...   \n",
              "\n",
              "                                            processed_amr  \\\n",
              "0       ( write-01     :ARG1 ( article)     :time ( da...   \n",
              "1       ( urge-01     :ARG0 ( person                pe...   \n",
              "2       ( and     :op1 ( beat-03              :ARG0 ( ...   \n",
              "3       ( close-01     :ARG1 ( road)     :duration ( m...   \n",
              "4                  ( advise-01     :ARG2 ( slow-down-03))   \n",
              "...                                                   ...   \n",
              "100454  ( commit-02     :ARG0 ( person               :...   \n",
              "100455  ( decline-01     :ARG1 ( number               ...   \n",
              "100456  ( experience-01     :ARG0 ( person            ...   \n",
              "100457  ( show-01     :ARG0 ( figure)     :ARG1 ( have...   \n",
              "100458  ( welcome-01     :ARG0 ( person               ...   \n",
              "\n",
              "                                          entity_mappings  \n",
              "0                                                      {}  \n",
              "1                                  {'person_0': 'Gillum'}  \n",
              "2       {'organization_0': 'Carlton', 'organization_1'...  \n",
              "3                                                      {}  \n",
              "4                                                      {}  \n",
              "...                                                   ...  \n",
              "100454                       {'location_15956': 'United'}  \n",
              "100455                            {'person_37066': 'New'}  \n",
              "100456                   {'person_37067': 'Bechtolsheim'}  \n",
              "100457                          {'location_15957': 'New'}  \n",
              "100458                        {'location_15958': 'China'}  \n",
              "\n",
              "[100459 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b982ecf3-9642-48d1-a88f-328571c94c9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "      <th>amr</th>\n",
              "      <th>processed_amr</th>\n",
              "      <th>entity_mappings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TOKYO, Dec 18 (Reuters) - Japan’s Shionogi &amp; C...</td>\n",
              "      <td>The article was written on December 18th.</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...</td>\n",
              "      <td>( write-01     :ARG1 ( article)     :time ( da...</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tallahassee Mayor and Democratic gubernatorial...</td>\n",
              "      <td>Gillum was on TV urging residents to stay out ...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...</td>\n",
              "      <td>( urge-01     :ARG0 ( person                pe...</td>\n",
              "      <td>{'person_0': 'Gillum'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MELBOURNE will look to avoid stumbling against...</td>\n",
              "      <td>Carlton beat Melbourne in 2016 and will attemp...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / and\\n    :op1 (z1 / beat-03\\n           ...</td>\n",
              "      <td>( and     :op1 ( beat-03              :ARG0 ( ...</td>\n",
              "      <td>{'organization_0': 'Carlton', 'organization_1'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>by Ted Raymond, Newstalk 580 CFRA A stretch of...</td>\n",
              "      <td>The road was closed for more than two hours af...</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...</td>\n",
              "      <td>( close-01     :ARG1 ( road)     :duration ( m...</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Drivers are reporting heavy traffic on the nor...</td>\n",
              "      <td>Its advisible to slow down</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))</td>\n",
              "      <td>( advise-01     :ARG2 ( slow-down-03))</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100454</th>\n",
              "      <td>A U.S. soldier accused of participating in the...</td>\n",
              "      <td>The Soldier committed these crimes in the unit...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / commit-02\\n    :ARG0 (z1 / person\\n     ...</td>\n",
              "      <td>( commit-02     :ARG0 ( person               :...</td>\n",
              "      <td>{'location_15956': 'United'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100455</th>\n",
              "      <td>Figures released today show that the number of...</td>\n",
              "      <td>there is a continuous decline in the number of...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / decline-01\\n    :ARG1 (z1 / number\\n    ...</td>\n",
              "      <td>( decline-01     :ARG1 ( number               ...</td>\n",
              "      <td>{'person_37066': 'New'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100456</th>\n",
              "      <td>For Bechtolsheim, who designed the prototype f...</td>\n",
              "      <td>Bechtolsheim is experienced with computers.</td>\n",
              "      <td>0</td>\n",
              "      <td>(z0 / experience-01\\n    :ARG0 (z1 / person\\n ...</td>\n",
              "      <td>( experience-01     :ARG0 ( person            ...</td>\n",
              "      <td>{'person_37067': 'Bechtolsheim'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100457</th>\n",
              "      <td>Figures released today show that the number of...</td>\n",
              "      <td>figures shows New Zealanders are the lowest or...</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / show-01\\n    :ARG0 (z1 / figure)\\n    :A...</td>\n",
              "      <td>( show-01     :ARG0 ( figure)     :ARG1 ( have...</td>\n",
              "      <td>{'location_15957': 'New'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100458</th>\n",
              "      <td>Tenzin Gyatso, the 14th Dalai Lama, has vowed ...</td>\n",
              "      <td>Some tibetans welcome Chinese rule.</td>\n",
              "      <td>1</td>\n",
              "      <td>(z0 / welcome-01\\n    :ARG0 (z1 / person\\n    ...</td>\n",
              "      <td>( welcome-01     :ARG0 ( person               ...</td>\n",
              "      <td>{'location_15958': 'China'}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100459 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b982ecf3-9642-48d1-a88f-328571c94c9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b982ecf3-9642-48d1-a88f-328571c94c9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b982ecf3-9642-48d1-a88f-328571c94c9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e4fcc1ba-e256-4613-9d59-4233b62eaffb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4fcc1ba-e256-4613-9d59-4233b62eaffb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e4fcc1ba-e256-4613-9d59-4233b62eaffb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e4650d80-618a-4c90-9c7f-51c7421cb939\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e4650d80-618a-4c90-9c7f-51c7421cb939 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print statistics\n",
        "print(\"\\nProcessing complete. Statistics:\")\n",
        "total_amrs = len(df)\n",
        "amrs_with_entities = sum(1 for m in entity_mappings if m)\n",
        "print(f\"Total AMRs processed: {total_amrs}\")\n",
        "print(f\"AMRs with recognized entities: {amrs_with_entities}\")\n",
        "print(f\"Percentage with entities: {(amrs_with_entities/total_amrs)*100:.2f}%\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nExample processed AMRs:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"Original AMR:\")\n",
        "    print(df['amr'].iloc[i])\n",
        "    print(\"\\nProcessed AMR:\")\n",
        "    print(df['processed_amr'].iloc[i])\n",
        "    print(\"Entity Map:\")\n",
        "    print(df['entity_mappings'].iloc[i])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yzqcl-qxQhF",
        "outputId": "fec9a38f-2f00-4413-d643-097ec7e2aad2"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete. Statistics:\n",
            "Total AMRs processed: 100459\n",
            "AMRs with recognized entities: 48482\n",
            "Percentage with entities: 48.26%\n",
            "\n",
            "Example processed AMRs:\n",
            "\n",
            "Example 1:\n",
            "Original AMR:\n",
            "(z0 / write-01\n",
            "    :ARG1 (z1 / article)\n",
            "    :time (z2 / date-entity\n",
            "              :day 18\n",
            "              :month 12))\n",
            "\n",
            "Processed AMR:\n",
            "( write-01     :ARG1 ( article)     :time ( date-entity               :day day_18               :month month_12))\n",
            "Entity Map:\n",
            "{}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Original AMR:\n",
            "(z0 / urge-01\n",
            "    :ARG0 (z1 / person\n",
            "              :name (z2 / name\n",
            "                        :op1 \"Gillum\")\n",
            "              :medium (z3 / television))\n",
            "    :ARG1 (z4 / person\n",
            "              :ARG0-of (z5 / reside-01))\n",
            "    :ARG2 (z6 / stay-01\n",
            "              :ARG1 z4\n",
            "              :ARG3 (z7 / out-06\n",
            "                        :ARG1 z4\n",
            "                        :ARG2 (z8 / storm))))\n",
            "\n",
            "Processed AMR:\n",
            "( urge-01     :ARG0 ( person                person_0               :medium ( television))     :ARG1 ( person               :ARG0-of ( reside-01))     :ARG2 ( stay-01               :ARG1               :ARG3 ( out-06                         :ARG1                         :ARG2 ( storm))))\n",
            "Entity Map:\n",
            "{'person_0': 'Gillum'}\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Original AMR:\n",
            "(z0 / and\n",
            "    :op1 (z1 / beat-03\n",
            "             :ARG0 (z2 / team\n",
            "                       :name (z3 / name\n",
            "                                 :op1 \"Carlton\"))\n",
            "             :ARG1 (z4 / team\n",
            "                       :name (z5 / name\n",
            "                                 :op1 \"Melbourne\"))\n",
            "             :time (z6 / date-entity\n",
            "                       :year 2016))\n",
            "    :op2 (z7 / attempt-01\n",
            "             :ARG0 z2\n",
            "             :ARG1 (z8 / beat-03\n",
            "                       :ARG0 z2\n",
            "                       :ARG1 z4\n",
            "                       :mod (z9 / again)\n",
            "                       :time (z10 / year\n",
            "                                  :mod (z11 / this)))))\n",
            "\n",
            "Processed AMR:\n",
            "( and     :op1 ( beat-03              :ARG0 ( team                         organization_0)              :ARG1 ( team                         organization_1)              :time ( date-entity                        :year year_2016))     :op2 ( attempt-01              :ARG0              :ARG1 ( beat-03                        :ARG0                        :ARG1                        :mod ( again)                        :time ( year                                   :mod ( this)))))\n",
            "Entity Map:\n",
            "{'organization_0': 'Carlton', 'organization_1': 'Melbourne'}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('processed_amrs.csv', index=False)\n",
        "print(\"Data saved to 'processed_amrs.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgnXEM6v0gft",
        "outputId": "8f082152-ef18-467c-c670-97c68fd42283"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'processed_amrs.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6e--AbcaUu0W"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AMRDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=512):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        processed_amr = row['processed_amr']\n",
        "        premise = row['premise']\n",
        "\n",
        "        # Prepare inputs (processed AMR)\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            processed_amr,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Prepare targets (premise text)\n",
        "        targets = self.tokenizer.encode_plus(\n",
        "            premise,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': targets['input_ids'].squeeze()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "uy8NXxtM0TnX"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "Q-7Hgtc_0XhQ"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the processed data\n",
        "    df = pd.read_csv('processed_amrs.csv')\n",
        "    print(f\"Loaded dataset with {len(df)} examples\")\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Split data into train and validation\n",
        "    train_size = int(0.8 * len(df))\n",
        "    train_df = df[:train_size]\n",
        "    val_df = df[train_size:]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = AMRDataset(train_df, tokenizer)\n",
        "    val_dataset = AMRDataset(val_df, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Training settings\n",
        "    num_epochs = 3\n",
        "    warmup_steps = 0\n",
        "    total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validate\n",
        "        val_loss = validate(model, val_dataloader, device)\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614,
          "referenced_widgets": [
            "7bb6a55b33cc49c4806f7a118f38b121",
            "5eff40e48f87481eacf3bdadc88dec32",
            "65f1af273ff84b3f93bfba5847bf96e1",
            "5a3cd10ca2ee46809d45e52dc5233e65",
            "d665e75b7ab844c181e9f2a3adbd4f15",
            "7687303efe4243feb53c394b21410ec1",
            "07c791e24bab4c30b9c54c6c3f4b429a",
            "6ae5b4c5995c4c1782f5da13339f097d",
            "df489a1bf9354c2484ef05281bebbff1",
            "6667cdb929444a828271e559b26d363d",
            "8c3fc3f9510f4b79aa18ddd4f9febfb6",
            "afad1e39b6c74be699f337a774a483cd",
            "012e490861f349a29af548c8f92fa143",
            "4e719b0f04334d769f4b83ed8324491e",
            "baebc33e6a3843e78613de4220bafaea",
            "7ac7140da71f443a9ee1bd3ef07f80f0",
            "affed3b859364e03baf55040f7ad7df8",
            "d8737b02971c4ef09c85092c5a257e2e",
            "0442b152e3bd4c1d9352984846828ba4",
            "a6b7c7a94cb6422184eaf7093f52dcb4",
            "8b1bd1528596433392c866362ad9fb19",
            "eda26f3fc3df4dc89a6884fd70f80c42",
            "b9ebc67d1f7241fbbc9a3b0487bf3e61",
            "fe78da53d7094cfa81ddaa6e606291bf",
            "bc9a8fdf8484423ca6a4bcbe4a6b95ba",
            "da33ca40ff7144e8a3c688cb0e469e6d",
            "bc5aea863d33461aac97b2caa10cdf81",
            "c5879cc67738418c8ccec038bd5dae59",
            "7310384979b94d2cb004857796cfd78e",
            "5955ae0415b54fa9bbbfa5e8038a7edc",
            "046ff847be5b4ea981c9efcc9c1ef12e",
            "831e20812f0e41ddad76f0a740a97149",
            "99ed042ffefe45dbb65354e566db0914",
            "5384daecea604e9198a5efa48da27410",
            "fdfd8553e2ca42e59e3826fa658dafc6",
            "4aad8c87ae564553b59407802b126617",
            "d66b5f962ba7467fa1d2accbdee4fc0b",
            "2796228ebb084c1ab0684e29541eb3b7",
            "09237c780a6b4fda8b7bdbed2a765f01",
            "106375c8055643f7937050e43ad5f292",
            "b4845c19cbc04ca9971881bbbc68ca74",
            "077574d6df19478ba04d7dac95aa9371",
            "9266600848234606ac48c1003a533927",
            "92f732ee7c904cebb09c43e1365c05ad",
            "c3d84570f5c04a70866227374392e773",
            "d7787b7396d14692a45c2d14cc5341ce",
            "cf779795be584713a4ed8075b5b249a0",
            "ad3df42861ff471da58589f794751025",
            "a159a72bb32e41c2a790f873412825f7",
            "8b13be7532d64223a4f4e9bbdfbcd877",
            "8be9cca26236493eb50d1bbe4b6674ec",
            "da36e38de4254c8fad157859675996c1",
            "1191fb069199482892c2cbfcd8c49839",
            "7d5fcb5d4e5249168f8edfd3660ace34",
            "a5f831e8b3d84406ac4742f646df8696"
          ]
        },
        "id": "5UXa58aF0bdn",
        "outputId": "0bf7d4cb-b6c3-4b95-b246-58c6ba25075f"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 100459 examples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bb6a55b33cc49c4806f7a118f38b121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afad1e39b6c74be699f337a774a483cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9ebc67d1f7241fbbc9a3b0487bf3e61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5384daecea604e9198a5efa48da27410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3d84570f5c04a70866227374392e773"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/10046 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Training: 100%|██████████| 10046/10046 [1:27:31<00:00,  1.91it/s, loss=0.435]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.4954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2512/2512 [07:07<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.4410\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 10046/10046 [1:27:34<00:00,  1.91it/s, loss=0.364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.3914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2512/2512 [07:07<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.4347\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 10046/10046 [1:27:31<00:00,  1.91it/s, loss=0.507]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.3576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2512/2512 [07:06<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.4335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "def calculate_bleu_score():\n",
        "    # Load the processed data\n",
        "    df = pd.read_csv('processed_amrs.csv')\n",
        "    print(f\"Loaded dataset with {len(df)} examples\")\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "    # Load the trained model\n",
        "    checkpoint = torch.load('checkpoint_epoch_3.pt', weights_only=True)  # Load the last checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set device and model to evaluation mode\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model = model.half()  # Convert to FP16 for A100\n",
        "\n",
        "    # Split data (use the same validation split as during training)\n",
        "    train_size = int(0.8 * len(df))\n",
        "    val_df = df[train_size:]\n",
        "\n",
        "    # Create validation dataset and dataloader\n",
        "    val_dataset = AMRDataset(val_df, tokenizer)\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=32,  # Larger batch size for A100\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    def calculate_bleu(model, tokenizer, dataloader, device):\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_references = []\n",
        "        smooth = SmoothingFunction()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):  # Proper autocast for A100\n",
        "                for batch in tqdm(dataloader, desc='Generating predictions'):\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "\n",
        "                    outputs = model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_length=256,\n",
        "                        num_beams=4,\n",
        "                        length_penalty=2.0,\n",
        "                        early_stopping=True,\n",
        "                        max_new_tokens=150,\n",
        "                        no_repeat_ngram_size=3\n",
        "                    )\n",
        "\n",
        "                    predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "                    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "                    predictions = [nltk.word_tokenize(pred.lower()) for pred in predictions]\n",
        "                    references = [[nltk.word_tokenize(ref.lower())] for ref in references]\n",
        "\n",
        "                    all_predictions.extend(predictions)\n",
        "                    all_references.extend(references)\n",
        "\n",
        "                    # Clear cache periodically\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        bleu_score = corpus_bleu(all_references, all_predictions, smoothing_function=smooth.method1)\n",
        "        return bleu_score\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    print(\"Starting BLEU score calculation...\")\n",
        "    bleu_score = calculate_bleu(model, tokenizer, val_dataloader, device)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    # Clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Run the BLEU score calculation\n",
        "calculate_bleu_score()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSRTu_JA7D3j",
        "outputId": "8c59bbbb-f851-4054-da0d-5f66045f47eb"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 100459 examples\n",
            "Starting BLEU score calculation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/628 [00:00<?, ?it/s]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   0%|          | 1/628 [00:10<1:54:06, 10.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   0%|          | 2/628 [00:21<1:50:13, 10.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   0%|          | 3/628 [00:31<1:46:20, 10.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|          | 4/628 [00:40<1:42:08,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|          | 5/628 [00:49<1:40:19,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|          | 6/628 [00:58<1:37:44,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|          | 7/628 [01:09<1:41:14,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|▏         | 8/628 [01:19<1:42:18,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   1%|▏         | 9/628 [01:29<1:42:02,  9.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 10/628 [01:37<1:37:38,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 11/628 [01:47<1:38:54,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 12/628 [01:56<1:36:19,  9.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 13/628 [02:05<1:36:23,  9.40s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 14/628 [02:17<1:41:49,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   2%|▏         | 15/628 [02:26<1:40:01,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 16/628 [02:36<1:40:52,  9.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 17/628 [02:46<1:41:03,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 18/628 [02:59<1:50:17, 10.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 19/628 [03:10<1:51:25, 10.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 20/628 [03:20<1:47:24, 10.60s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   3%|▎         | 21/628 [03:30<1:45:38, 10.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▎         | 22/628 [03:39<1:40:12,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▎         | 23/628 [03:51<1:45:49, 10.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▍         | 24/628 [04:00<1:42:17, 10.16s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▍         | 25/628 [04:10<1:41:04, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▍         | 26/628 [04:19<1:37:33,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▍         | 27/628 [04:28<1:34:16,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   4%|▍         | 28/628 [04:36<1:32:13,  9.22s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▍         | 29/628 [04:47<1:35:43,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▍         | 30/628 [04:56<1:34:29,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▍         | 31/628 [05:06<1:34:20,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▌         | 32/628 [05:15<1:33:00,  9.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▌         | 33/628 [05:24<1:32:54,  9.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   5%|▌         | 34/628 [05:34<1:34:17,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▌         | 35/628 [05:43<1:33:03,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▌         | 36/628 [05:53<1:33:56,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▌         | 37/628 [06:03<1:35:35,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▌         | 38/628 [06:13<1:35:34,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▌         | 39/628 [06:23<1:35:58,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   6%|▋         | 40/628 [06:32<1:34:55,  9.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 41/628 [06:42<1:34:30,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 42/628 [06:50<1:30:31,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 43/628 [07:01<1:33:54,  9.63s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 44/628 [07:12<1:38:26, 10.11s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 45/628 [07:21<1:34:57,  9.77s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 46/628 [07:30<1:33:46,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   7%|▋         | 47/628 [07:40<1:35:01,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 48/628 [07:50<1:33:10,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 49/628 [08:00<1:34:21,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 50/628 [08:09<1:33:25,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 51/628 [08:18<1:31:03,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 52/628 [08:27<1:29:35,  9.33s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   8%|▊         | 53/628 [08:37<1:32:08,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▊         | 54/628 [08:47<1:30:29,  9.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▉         | 55/628 [08:57<1:32:35,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▉         | 56/628 [09:07<1:32:53,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▉         | 57/628 [09:16<1:31:52,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▉         | 58/628 [09:25<1:30:28,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:   9%|▉         | 59/628 [09:34<1:29:20,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|▉         | 60/628 [09:43<1:26:57,  9.19s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|▉         | 61/628 [09:52<1:25:32,  9.05s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|▉         | 62/628 [10:03<1:32:08,  9.77s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|█         | 63/628 [10:12<1:29:48,  9.54s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|█         | 64/628 [10:23<1:34:13, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  10%|█         | 65/628 [10:32<1:30:49,  9.68s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█         | 66/628 [10:43<1:32:26,  9.87s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█         | 67/628 [10:52<1:30:48,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█         | 68/628 [11:03<1:35:03, 10.19s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█         | 69/628 [11:12<1:30:30,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█         | 70/628 [11:22<1:31:07,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█▏        | 71/628 [11:32<1:30:50,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  11%|█▏        | 72/628 [11:41<1:30:25,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 73/628 [11:51<1:31:05,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 74/628 [12:01<1:30:04,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 75/628 [12:11<1:30:15,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 76/628 [12:20<1:27:09,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 77/628 [12:29<1:27:50,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  12%|█▏        | 78/628 [12:40<1:30:19,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 79/628 [12:51<1:33:47, 10.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 80/628 [13:01<1:32:44, 10.15s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 81/628 [13:11<1:31:18, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 82/628 [13:19<1:27:29,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 83/628 [13:29<1:26:28,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  13%|█▎        | 84/628 [13:38<1:24:43,  9.34s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▎        | 85/628 [13:47<1:23:58,  9.28s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▎        | 86/628 [13:56<1:23:33,  9.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▍        | 87/628 [14:07<1:27:58,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▍        | 88/628 [14:16<1:27:18,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▍        | 89/628 [14:26<1:27:07,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▍        | 90/628 [14:36<1:27:20,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  14%|█▍        | 91/628 [14:45<1:25:40,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▍        | 92/628 [14:55<1:27:35,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▍        | 93/628 [15:04<1:24:28,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▍        | 94/628 [15:14<1:24:12,  9.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▌        | 95/628 [15:25<1:28:23,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▌        | 96/628 [15:35<1:29:53, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  15%|█▌        | 97/628 [15:45<1:28:03,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▌        | 98/628 [15:54<1:27:02,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▌        | 99/628 [16:04<1:26:16,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▌        | 100/628 [16:14<1:25:42,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▌        | 101/628 [16:23<1:23:43,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▌        | 102/628 [16:32<1:22:33,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  16%|█▋        | 103/628 [16:42<1:23:23,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 104/628 [16:51<1:22:58,  9.50s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 105/628 [17:01<1:24:13,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 106/628 [17:11<1:24:33,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 107/628 [17:21<1:24:14,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 108/628 [17:30<1:23:01,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  17%|█▋        | 109/628 [17:39<1:21:34,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 110/628 [17:50<1:24:36,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 111/628 [17:59<1:24:36,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 112/628 [18:08<1:22:19,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 113/628 [18:19<1:25:13,  9.93s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 114/628 [18:29<1:24:37,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 115/628 [18:38<1:22:54,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  18%|█▊        | 116/628 [18:49<1:25:26, 10.01s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▊        | 117/628 [18:59<1:24:46,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▉        | 118/628 [19:08<1:22:01,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▉        | 119/628 [19:18<1:22:30,  9.73s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▉        | 120/628 [19:27<1:20:03,  9.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▉        | 121/628 [19:36<1:20:00,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  19%|█▉        | 122/628 [19:46<1:19:54,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|█▉        | 123/628 [19:56<1:21:43,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|█▉        | 124/628 [20:07<1:24:48, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|█▉        | 125/628 [20:17<1:26:04, 10.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|██        | 126/628 [20:27<1:23:20,  9.96s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|██        | 127/628 [20:36<1:21:03,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  20%|██        | 128/628 [20:46<1:21:53,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██        | 129/628 [20:55<1:19:40,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██        | 130/628 [21:05<1:20:23,  9.68s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██        | 131/628 [21:16<1:23:41, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██        | 132/628 [21:26<1:23:09, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██        | 133/628 [21:35<1:20:14,  9.73s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██▏       | 134/628 [21:45<1:21:11,  9.86s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  21%|██▏       | 135/628 [21:54<1:18:48,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 136/628 [22:03<1:18:24,  9.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 137/628 [22:13<1:18:44,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 138/628 [22:25<1:23:51, 10.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 139/628 [22:35<1:22:37, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 140/628 [22:45<1:21:52, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  22%|██▏       | 141/628 [22:54<1:20:11,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 142/628 [23:04<1:18:44,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 143/628 [23:13<1:17:47,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 144/628 [23:23<1:19:11,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 145/628 [23:35<1:23:28, 10.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 146/628 [23:45<1:22:53, 10.32s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  23%|██▎       | 147/628 [23:54<1:19:09,  9.87s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▎       | 148/628 [24:04<1:20:06, 10.01s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▎       | 149/628 [24:14<1:20:11, 10.04s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▍       | 150/628 [24:24<1:18:52,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▍       | 151/628 [24:33<1:16:38,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▍       | 152/628 [24:43<1:16:29,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  24%|██▍       | 153/628 [24:53<1:18:31,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▍       | 154/628 [25:02<1:15:37,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▍       | 155/628 [25:12<1:16:53,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▍       | 156/628 [25:23<1:18:47, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▌       | 157/628 [25:33<1:19:07, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▌       | 158/628 [25:42<1:16:33,  9.77s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▌       | 159/628 [25:50<1:13:03,  9.35s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  25%|██▌       | 160/628 [26:00<1:14:19,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▌       | 161/628 [26:09<1:12:54,  9.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▌       | 162/628 [26:18<1:11:58,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▌       | 163/628 [26:29<1:14:52,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▌       | 164/628 [26:38<1:13:58,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▋       | 165/628 [26:48<1:13:34,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  26%|██▋       | 166/628 [26:57<1:13:35,  9.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 167/628 [27:09<1:17:55, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 168/628 [27:20<1:19:32, 10.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 169/628 [27:28<1:15:02,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 170/628 [27:37<1:12:49,  9.54s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 171/628 [27:49<1:17:33, 10.18s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  27%|██▋       | 172/628 [27:59<1:16:30, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 173/628 [28:07<1:13:11,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 174/628 [28:16<1:11:31,  9.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 175/628 [28:28<1:16:59, 10.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 176/628 [28:38<1:15:25, 10.01s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 177/628 [28:48<1:15:55, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  28%|██▊       | 178/628 [28:58<1:14:13,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▊       | 179/628 [29:08<1:14:49, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▊       | 180/628 [29:17<1:12:01,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▉       | 181/628 [29:26<1:10:57,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▉       | 182/628 [29:35<1:10:04,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▉       | 183/628 [29:47<1:15:39, 10.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▉       | 184/628 [29:57<1:14:20, 10.05s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  29%|██▉       | 185/628 [30:06<1:12:36,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|██▉       | 186/628 [30:15<1:10:24,  9.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|██▉       | 187/628 [30:24<1:09:29,  9.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|██▉       | 188/628 [30:33<1:08:36,  9.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|███       | 189/628 [30:43<1:08:08,  9.31s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|███       | 190/628 [30:54<1:12:59, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  30%|███       | 191/628 [31:04<1:12:24,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███       | 192/628 [31:18<1:20:24, 11.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███       | 193/628 [31:27<1:16:16, 10.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███       | 194/628 [31:37<1:15:16, 10.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███       | 195/628 [31:48<1:17:17, 10.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███       | 196/628 [31:58<1:14:09, 10.30s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  31%|███▏      | 197/628 [32:08<1:14:18, 10.34s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 198/628 [32:19<1:14:50, 10.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 199/628 [32:29<1:14:19, 10.40s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 200/628 [32:39<1:12:38, 10.18s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 201/628 [32:47<1:08:42,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 202/628 [32:57<1:07:40,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 203/628 [33:08<1:10:45,  9.99s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  32%|███▏      | 204/628 [33:18<1:11:41, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 205/628 [33:27<1:09:01,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 206/628 [33:36<1:06:53,  9.51s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 207/628 [33:45<1:06:03,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 208/628 [33:54<1:05:45,  9.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 209/628 [34:04<1:05:33,  9.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  33%|███▎      | 210/628 [34:14<1:06:12,  9.50s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▎      | 211/628 [34:24<1:07:46,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▍      | 212/628 [34:34<1:08:00,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▍      | 213/628 [34:44<1:08:18,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▍      | 214/628 [34:53<1:06:44,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▍      | 215/628 [35:02<1:05:21,  9.50s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  34%|███▍      | 216/628 [35:13<1:06:51,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▍      | 217/628 [35:22<1:05:25,  9.55s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▍      | 218/628 [35:33<1:09:28, 10.17s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▍      | 219/628 [35:43<1:09:05, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▌      | 220/628 [35:53<1:07:10,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▌      | 221/628 [36:03<1:08:20, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  35%|███▌      | 222/628 [36:12<1:06:03,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▌      | 223/628 [36:23<1:07:26,  9.99s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▌      | 224/628 [36:32<1:06:21,  9.86s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▌      | 225/628 [36:41<1:04:21,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▌      | 226/628 [36:51<1:03:50,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▌      | 227/628 [37:00<1:02:43,  9.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▋      | 228/628 [37:10<1:04:21,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  36%|███▋      | 229/628 [37:21<1:07:25, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 230/628 [37:31<1:05:45,  9.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 231/628 [37:40<1:05:01,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 232/628 [37:50<1:04:20,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 233/628 [38:00<1:04:49,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 234/628 [38:09<1:02:57,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  37%|███▋      | 235/628 [38:18<1:01:27,  9.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 236/628 [38:27<1:00:31,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 237/628 [38:37<1:02:26,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 238/628 [38:47<1:04:02,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 239/628 [38:59<1:06:57, 10.33s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 240/628 [39:09<1:06:14, 10.24s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  38%|███▊      | 241/628 [39:18<1:03:21,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▊      | 242/628 [39:28<1:03:04,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▊      | 243/628 [39:37<1:02:27,  9.73s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▉      | 244/628 [39:47<1:02:28,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▉      | 245/628 [39:58<1:05:14, 10.22s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▉      | 246/628 [40:07<1:02:28,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▉      | 247/628 [40:17<1:02:10,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  39%|███▉      | 248/628 [40:26<1:01:33,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|███▉      | 249/628 [40:37<1:03:04,  9.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|███▉      | 250/628 [40:46<1:01:24,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|███▉      | 251/628 [40:57<1:02:34,  9.96s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|████      | 252/628 [41:07<1:02:28,  9.97s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|████      | 253/628 [41:16<1:00:41,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  40%|████      | 254/628 [41:28<1:05:26, 10.50s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████      | 255/628 [41:37<1:01:46,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████      | 256/628 [41:49<1:05:55, 10.63s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████      | 257/628 [41:59<1:04:39, 10.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████      | 258/628 [42:10<1:05:56, 10.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████      | 259/628 [42:20<1:04:06, 10.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  41%|████▏     | 260/628 [42:29<1:01:43, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 261/628 [42:40<1:03:22, 10.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 262/628 [42:50<1:01:26, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 263/628 [43:00<1:01:29, 10.11s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 264/628 [43:10<1:02:07, 10.24s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 265/628 [43:21<1:02:28, 10.33s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  42%|████▏     | 266/628 [43:31<1:01:36, 10.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 267/628 [43:41<1:00:30, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 268/628 [43:50<59:41,  9.95s/it]  Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 269/628 [44:00<59:21,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 270/628 [44:11<1:01:29, 10.31s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 271/628 [44:20<58:42,  9.87s/it]  Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 272/628 [44:31<1:00:05, 10.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  43%|████▎     | 273/628 [44:41<59:08,  9.99s/it]  Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▎     | 274/628 [44:49<56:52,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▍     | 275/628 [44:59<56:28,  9.60s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▍     | 276/628 [45:09<56:41,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▍     | 277/628 [45:19<57:34,  9.84s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▍     | 278/628 [45:28<56:38,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  44%|████▍     | 279/628 [45:40<58:56, 10.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▍     | 280/628 [45:49<57:49,  9.97s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▍     | 281/628 [45:59<56:59,  9.86s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▍     | 282/628 [46:09<57:47, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▌     | 283/628 [46:19<56:37,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▌     | 284/628 [46:28<55:52,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  45%|████▌     | 285/628 [46:37<54:29,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▌     | 286/628 [46:47<54:06,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▌     | 287/628 [46:56<53:01,  9.33s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▌     | 288/628 [47:05<53:22,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▌     | 289/628 [47:14<52:09,  9.23s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▌     | 290/628 [47:23<51:48,  9.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▋     | 291/628 [47:32<51:11,  9.11s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  46%|████▋     | 292/628 [47:42<51:54,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 293/628 [47:51<51:45,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 294/628 [48:02<54:18,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 295/628 [48:11<52:38,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 296/628 [48:20<53:08,  9.60s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 297/628 [48:30<52:40,  9.55s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  47%|████▋     | 298/628 [48:39<51:52,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 299/628 [48:48<50:04,  9.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 300/628 [48:57<50:50,  9.30s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 301/628 [49:06<49:29,  9.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 302/628 [49:14<48:05,  8.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 303/628 [49:23<48:28,  8.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  48%|████▊     | 304/628 [49:35<52:02,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▊     | 305/628 [49:45<52:39,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▊     | 306/628 [49:54<51:46,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▉     | 307/628 [50:04<52:34,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▉     | 308/628 [50:14<53:07,  9.96s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▉     | 309/628 [50:25<53:48, 10.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  49%|████▉     | 310/628 [50:34<51:29,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|████▉     | 311/628 [50:43<50:37,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|████▉     | 312/628 [50:53<50:36,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|████▉     | 313/628 [51:02<49:32,  9.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|█████     | 314/628 [51:12<51:02,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|█████     | 315/628 [51:22<50:53,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|█████     | 316/628 [51:31<50:21,  9.68s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  50%|█████     | 317/628 [51:41<50:14,  9.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████     | 318/628 [51:51<50:28,  9.77s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████     | 319/628 [52:01<51:11,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████     | 320/628 [52:12<51:50, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████     | 321/628 [52:22<51:28, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████▏    | 322/628 [52:32<50:36,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  51%|█████▏    | 323/628 [52:41<50:20,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 324/628 [52:51<49:20,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 325/628 [53:00<48:08,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 326/628 [53:10<48:31,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 327/628 [53:19<48:08,  9.60s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 328/628 [53:29<48:59,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  52%|█████▏    | 329/628 [53:39<47:57,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 330/628 [53:48<47:08,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 331/628 [53:57<46:56,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 332/628 [54:07<46:31,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 333/628 [54:17<48:13,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 334/628 [54:27<48:06,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  53%|█████▎    | 335/628 [54:37<47:46,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▎    | 336/628 [54:47<48:04,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▎    | 337/628 [54:57<47:57,  9.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▍    | 338/628 [55:08<49:17, 10.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▍    | 339/628 [55:18<49:50, 10.35s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▍    | 340/628 [55:27<47:33,  9.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▍    | 341/628 [55:37<47:10,  9.86s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  54%|█████▍    | 342/628 [55:47<47:02,  9.87s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▍    | 343/628 [55:57<46:36,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▍    | 344/628 [56:07<47:47, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▍    | 345/628 [56:18<47:42, 10.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▌    | 346/628 [56:27<46:59, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▌    | 347/628 [56:36<45:37,  9.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  55%|█████▌    | 348/628 [56:48<48:05, 10.30s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▌    | 349/628 [56:57<45:28,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▌    | 350/628 [57:05<43:29,  9.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▌    | 351/628 [57:14<42:19,  9.17s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▌    | 352/628 [57:25<44:42,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▌    | 353/628 [57:36<45:56, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  56%|█████▋    | 354/628 [57:45<44:55,  9.84s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 355/628 [57:55<44:50,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 356/628 [58:05<45:13,  9.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 357/628 [58:15<44:24,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 358/628 [58:24<44:05,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 359/628 [58:35<45:21, 10.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 360/628 [58:45<45:07, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  57%|█████▋    | 361/628 [58:54<43:50,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 362/628 [59:08<48:15, 10.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 363/628 [59:17<45:49, 10.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 364/628 [59:27<44:45, 10.17s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 365/628 [59:36<43:11,  9.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 366/628 [59:45<41:49,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  58%|█████▊    | 367/628 [59:54<40:56,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▊    | 368/628 [1:00:03<41:01,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▉    | 369/628 [1:00:12<39:43,  9.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▉    | 370/628 [1:00:22<40:48,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▉    | 371/628 [1:00:33<42:03,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▉    | 372/628 [1:00:42<40:55,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  59%|█████▉    | 373/628 [1:00:52<41:29,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|█████▉    | 374/628 [1:01:01<40:50,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|█████▉    | 375/628 [1:01:11<40:11,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|█████▉    | 376/628 [1:01:22<41:53,  9.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|██████    | 377/628 [1:01:32<41:46,  9.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|██████    | 378/628 [1:01:40<40:09,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  60%|██████    | 379/628 [1:01:49<39:02,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████    | 380/628 [1:02:01<41:24, 10.02s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████    | 381/628 [1:02:11<41:01,  9.97s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████    | 382/628 [1:02:20<40:28,  9.87s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████    | 383/628 [1:02:30<40:33,  9.93s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████    | 384/628 [1:02:39<39:31,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████▏   | 385/628 [1:02:51<41:33, 10.26s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  61%|██████▏   | 386/628 [1:03:01<40:36, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 387/628 [1:03:10<40:06,  9.99s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 388/628 [1:03:21<40:32, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 389/628 [1:03:29<38:32,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 390/628 [1:03:39<38:12,  9.63s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 391/628 [1:03:49<38:07,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  62%|██████▏   | 392/628 [1:03:58<37:51,  9.63s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 393/628 [1:04:09<38:52,  9.93s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 394/628 [1:04:18<37:55,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 395/628 [1:04:28<37:45,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 396/628 [1:04:38<37:55,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 397/628 [1:04:48<37:44,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  63%|██████▎   | 398/628 [1:04:58<37:42,  9.84s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▎   | 399/628 [1:05:07<36:44,  9.63s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▎   | 400/628 [1:05:16<36:16,  9.55s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▍   | 401/628 [1:05:27<37:10,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▍   | 402/628 [1:05:36<36:06,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▍   | 403/628 [1:05:47<37:46, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▍   | 404/628 [1:05:57<37:34, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  64%|██████▍   | 405/628 [1:06:08<38:04, 10.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▍   | 406/628 [1:06:17<36:50,  9.96s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▍   | 407/628 [1:06:27<37:19, 10.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▍   | 408/628 [1:06:38<37:57, 10.35s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▌   | 409/628 [1:06:50<39:00, 10.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▌   | 410/628 [1:07:01<39:53, 10.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  65%|██████▌   | 411/628 [1:07:11<38:28, 10.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▌   | 412/628 [1:07:21<37:02, 10.29s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▌   | 413/628 [1:07:30<35:37,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▌   | 414/628 [1:07:41<36:59, 10.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▌   | 415/628 [1:07:51<36:02, 10.15s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▌   | 416/628 [1:08:02<36:49, 10.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  66%|██████▋   | 417/628 [1:08:12<36:35, 10.40s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 418/628 [1:08:24<37:26, 10.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 419/628 [1:08:34<36:30, 10.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 420/628 [1:08:45<37:10, 10.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 421/628 [1:08:54<35:49, 10.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 422/628 [1:09:05<35:35, 10.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  67%|██████▋   | 423/628 [1:09:14<34:44, 10.17s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 424/628 [1:09:26<35:44, 10.51s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 425/628 [1:09:35<34:32, 10.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 426/628 [1:09:45<34:19, 10.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 427/628 [1:09:57<35:19, 10.55s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 428/628 [1:10:06<33:29, 10.05s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 429/628 [1:10:15<32:32,  9.81s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  68%|██████▊   | 430/628 [1:10:25<32:27,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▊   | 431/628 [1:10:35<32:17,  9.84s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▉   | 432/628 [1:10:44<31:14,  9.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▉   | 433/628 [1:10:53<30:40,  9.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▉   | 434/628 [1:11:03<31:16,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▉   | 435/628 [1:11:14<32:21, 10.06s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  69%|██████▉   | 436/628 [1:11:24<32:22, 10.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|██████▉   | 437/628 [1:11:34<31:27,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|██████▉   | 438/628 [1:11:44<31:39, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|██████▉   | 439/628 [1:11:55<32:27, 10.30s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|███████   | 440/628 [1:12:06<32:51, 10.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|███████   | 441/628 [1:12:15<31:12, 10.01s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  70%|███████   | 442/628 [1:12:24<30:40,  9.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████   | 443/628 [1:12:33<29:51,  9.68s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████   | 444/628 [1:12:42<29:04,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████   | 445/628 [1:12:53<29:46,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████   | 446/628 [1:13:02<28:52,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████   | 447/628 [1:13:11<28:46,  9.54s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████▏  | 448/628 [1:13:21<29:00,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  71%|███████▏  | 449/628 [1:13:30<27:46,  9.31s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 450/628 [1:13:39<27:54,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 451/628 [1:13:49<28:00,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 452/628 [1:13:58<27:17,  9.31s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 453/628 [1:14:07<26:30,  9.09s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 454/628 [1:14:16<26:42,  9.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  72%|███████▏  | 455/628 [1:14:26<26:44,  9.28s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 456/628 [1:14:35<26:30,  9.24s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 457/628 [1:14:44<25:59,  9.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 458/628 [1:14:54<27:04,  9.55s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 459/628 [1:15:05<28:06,  9.98s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 460/628 [1:15:15<28:13, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  73%|███████▎  | 461/628 [1:15:25<27:56, 10.04s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▎  | 462/628 [1:15:37<28:55, 10.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▎  | 463/628 [1:15:46<27:41, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▍  | 464/628 [1:15:55<26:17,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▍  | 465/628 [1:16:03<25:36,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▍  | 466/628 [1:16:12<25:00,  9.26s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  74%|███████▍  | 467/628 [1:16:21<24:43,  9.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▍  | 468/628 [1:16:31<25:06,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▍  | 469/628 [1:16:41<24:53,  9.40s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▍  | 470/628 [1:16:52<26:04,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▌  | 471/628 [1:17:01<25:14,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▌  | 472/628 [1:17:10<24:25,  9.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▌  | 473/628 [1:17:19<24:12,  9.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  75%|███████▌  | 474/628 [1:17:29<24:41,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▌  | 475/628 [1:17:38<24:10,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▌  | 476/628 [1:17:48<24:30,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▌  | 477/628 [1:17:59<25:01,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▌  | 478/628 [1:18:08<23:57,  9.58s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▋  | 479/628 [1:18:17<23:49,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  76%|███████▋  | 480/628 [1:18:28<24:15,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 481/628 [1:18:38<24:41, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 482/628 [1:18:48<23:48,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 483/628 [1:18:57<23:13,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 484/628 [1:19:06<22:46,  9.49s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 485/628 [1:19:15<22:32,  9.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  77%|███████▋  | 486/628 [1:19:26<23:32,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 487/628 [1:19:36<22:48,  9.70s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 488/628 [1:19:46<23:18,  9.99s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 489/628 [1:19:56<23:19, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 490/628 [1:20:05<22:26,  9.76s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 491/628 [1:20:15<22:07,  9.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  78%|███████▊  | 492/628 [1:20:26<22:51, 10.09s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▊  | 493/628 [1:20:36<22:40, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▊  | 494/628 [1:20:47<23:19, 10.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▉  | 495/628 [1:20:57<22:53, 10.33s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▉  | 496/628 [1:21:08<22:51, 10.39s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▉  | 497/628 [1:21:19<23:03, 10.56s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▉  | 498/628 [1:21:29<22:29, 10.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  79%|███████▉  | 499/628 [1:21:41<23:10, 10.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|███████▉  | 500/628 [1:21:50<22:17, 10.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|███████▉  | 501/628 [1:22:01<21:58, 10.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|███████▉  | 502/628 [1:22:09<20:52,  9.94s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|████████  | 503/628 [1:22:19<20:22,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|████████  | 504/628 [1:22:27<19:31,  9.44s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  80%|████████  | 505/628 [1:22:36<18:57,  9.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████  | 506/628 [1:22:46<18:47,  9.24s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████  | 507/628 [1:22:57<20:14, 10.04s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████  | 508/628 [1:23:08<20:22, 10.19s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████  | 509/628 [1:23:20<21:31, 10.85s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████  | 510/628 [1:23:32<22:05, 11.23s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  81%|████████▏ | 511/628 [1:23:42<20:46, 10.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 512/628 [1:23:51<19:32, 10.11s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 513/628 [1:24:02<20:12, 10.54s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 514/628 [1:24:13<20:21, 10.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 515/628 [1:24:23<19:30, 10.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 516/628 [1:24:34<20:03, 10.74s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 517/628 [1:24:43<18:54, 10.22s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  82%|████████▏ | 518/628 [1:24:55<19:17, 10.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 519/628 [1:25:06<19:21, 10.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 520/628 [1:25:15<18:14, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 521/628 [1:25:24<17:39,  9.90s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 522/628 [1:25:35<18:07, 10.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 523/628 [1:25:45<17:55, 10.24s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  83%|████████▎ | 524/628 [1:25:54<16:59,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▎ | 525/628 [1:26:03<16:15,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▍ | 526/628 [1:26:13<16:25,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▍ | 527/628 [1:26:22<15:55,  9.46s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▍ | 528/628 [1:26:31<15:27,  9.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▍ | 529/628 [1:26:41<15:37,  9.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  84%|████████▍ | 530/628 [1:26:49<15:10,  9.29s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▍ | 531/628 [1:26:59<15:13,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▍ | 532/628 [1:27:09<15:07,  9.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▍ | 533/628 [1:27:18<14:42,  9.29s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▌ | 534/628 [1:27:27<14:25,  9.21s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▌ | 535/628 [1:27:38<15:30, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  85%|████████▌ | 536/628 [1:27:47<14:44,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▌ | 537/628 [1:27:56<14:18,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▌ | 538/628 [1:28:06<14:07,  9.42s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▌ | 539/628 [1:28:14<13:43,  9.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▌ | 540/628 [1:28:23<13:26,  9.17s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▌ | 541/628 [1:28:34<13:48,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▋ | 542/628 [1:28:43<13:26,  9.38s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  86%|████████▋ | 543/628 [1:28:52<13:21,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 544/628 [1:29:03<13:43,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 545/628 [1:29:12<13:02,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 546/628 [1:29:20<12:29,  9.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 547/628 [1:29:30<12:36,  9.34s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 548/628 [1:29:39<12:32,  9.40s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  87%|████████▋ | 549/628 [1:29:49<12:23,  9.41s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 550/628 [1:29:59<12:29,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 551/628 [1:30:13<13:57, 10.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 552/628 [1:30:22<13:00, 10.27s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 553/628 [1:30:32<12:44, 10.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 554/628 [1:30:42<12:45, 10.34s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  88%|████████▊ | 555/628 [1:30:52<12:15, 10.08s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▊ | 556/628 [1:31:05<13:05, 10.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▊ | 557/628 [1:31:14<12:21, 10.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▉ | 558/628 [1:31:24<12:09, 10.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▉ | 559/628 [1:31:34<11:37, 10.12s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▉ | 560/628 [1:31:43<11:03,  9.75s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▉ | 561/628 [1:31:51<10:31,  9.43s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  89%|████████▉ | 562/628 [1:32:00<10:10,  9.25s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|████████▉ | 563/628 [1:32:09<09:57,  9.19s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|████████▉ | 564/628 [1:32:21<10:28,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|████████▉ | 565/628 [1:32:30<10:12,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|█████████ | 566/628 [1:32:39<09:53,  9.57s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|█████████ | 567/628 [1:32:50<10:02,  9.88s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  90%|█████████ | 568/628 [1:33:00<09:54,  9.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████ | 569/628 [1:33:10<09:53, 10.07s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████ | 570/628 [1:33:21<09:51, 10.19s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████ | 571/628 [1:33:30<09:27,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████ | 572/628 [1:33:40<09:18,  9.97s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████ | 573/628 [1:33:51<09:28, 10.34s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  91%|█████████▏| 574/628 [1:34:01<09:02, 10.04s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 575/628 [1:34:10<08:45,  9.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 576/628 [1:34:20<08:29,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 577/628 [1:34:29<08:10,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 578/628 [1:34:39<08:00,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 579/628 [1:34:50<08:16, 10.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  92%|█████████▏| 580/628 [1:35:00<08:05, 10.11s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 581/628 [1:35:11<08:07, 10.37s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 582/628 [1:35:22<08:01, 10.47s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 583/628 [1:35:32<07:46, 10.36s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 584/628 [1:35:41<07:25, 10.14s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 585/628 [1:35:51<07:07,  9.95s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 586/628 [1:36:01<06:54,  9.87s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  93%|█████████▎| 587/628 [1:36:10<06:38,  9.73s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▎| 588/628 [1:36:19<06:21,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▍| 589/628 [1:36:29<06:21,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▍| 590/628 [1:36:39<06:06,  9.66s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▍| 591/628 [1:36:48<05:56,  9.65s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▍| 592/628 [1:36:58<05:48,  9.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  94%|█████████▍| 593/628 [1:37:07<05:33,  9.53s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▍| 594/628 [1:37:19<05:43, 10.10s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▍| 595/628 [1:37:28<05:27,  9.92s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▍| 596/628 [1:37:38<05:14,  9.83s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▌| 597/628 [1:37:47<04:57,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▌| 598/628 [1:37:57<04:48,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  95%|█████████▌| 599/628 [1:38:06<04:40,  9.67s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▌| 600/628 [1:38:16<04:30,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▌| 601/628 [1:38:26<04:25,  9.82s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▌| 602/628 [1:38:36<04:11,  9.69s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▌| 603/628 [1:38:45<04:00,  9.62s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▌| 604/628 [1:38:55<03:51,  9.64s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▋| 605/628 [1:39:04<03:38,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  96%|█████████▋| 606/628 [1:39:12<03:22,  9.20s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 607/628 [1:39:23<03:20,  9.54s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 608/628 [1:39:32<03:11,  9.59s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 609/628 [1:39:42<02:59,  9.45s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 610/628 [1:39:51<02:51,  9.51s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 611/628 [1:40:02<02:46,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  97%|█████████▋| 612/628 [1:40:11<02:36,  9.77s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 613/628 [1:40:20<02:22,  9.48s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 614/628 [1:40:31<02:19,  9.97s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 615/628 [1:40:41<02:08,  9.89s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 616/628 [1:40:51<01:57,  9.80s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 617/628 [1:41:01<01:50, 10.03s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  98%|█████████▊| 618/628 [1:41:10<01:36,  9.61s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▊| 619/628 [1:41:20<01:28,  9.78s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▊| 620/628 [1:41:31<01:21, 10.13s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▉| 621/628 [1:41:40<01:08,  9.79s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▉| 622/628 [1:41:49<00:58,  9.71s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▉| 623/628 [1:42:00<00:49,  9.93s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions:  99%|█████████▉| 624/628 [1:42:10<00:40, 10.00s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions: 100%|█████████▉| 625/628 [1:42:19<00:29,  9.72s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions: 100%|█████████▉| 626/628 [1:42:28<00:19,  9.52s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions: 100%|█████████▉| 627/628 [1:42:39<00:09,  9.91s/it]Both `max_new_tokens` (=150) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Generating predictions: 100%|██████████| 628/628 [1:42:48<00:00,  9.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.0195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbRchSGl7Xzv"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BJky3zpmZYE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}